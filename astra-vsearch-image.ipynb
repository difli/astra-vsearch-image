{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c69242",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/difli/astra-vsearch-image/blob/main/astra-vsearch-image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05e3f2",
   "metadata": {},
   "source": [
    "# astra-vsearch-image\n",
    "Jupyter notebook for image search powered by [Astra Vector Search](https://docs.datastax.com/en/astra-serverless/docs/vector-search/overview.html) and OpenAI [CLIP Model](https://github.com/openai/CLIP).\n",
    "\n",
    "## OpenAI CLIP Model\n",
    "CLIP, or \"Contrastive Language-Image Pretraining\", is an artificial intelligence model developed by OpenAI. The model is trained to understand and associate images with natural language by using a vast number of images and their associated textual descriptions. CLIP can perform tasks such as generating textual descriptions of images or finding images based on given text.\n",
    "\n",
    "## Astra Vector Search\n",
    "Astra vector search enables developers to search a database by context or meaning rather than keywords or literal values. This is done by using “embeddings”. Embeddings are a type of representation used in machine learning where high-dimensional or complex data is mapped onto vectors in a lower-dimensional space. These vectors capture the semantic properties of the input data, meaning that similar data points have similar embeddings. The CLIP model is used to generate the embeddings in this demo.\n",
    "\n",
    "## Demo Summary\n",
    "- The CLIP model generates embeddings for each image \n",
    "- Metdata and embeddings for each image are stored in Astra DB. \n",
    "- The embeddings are stored in a Astra DB column of Type Vector.  \n",
    "- The idea of the demo is to find with the help of Astra Vector Search the image of the house with a swimming pool.\n",
    "- The CLIP model generates embeddings based on the search string: \"a house with a swimming pool\". \n",
    "- This embedding is used in the query to find the image that has similar embeddings as the search string.\n",
    "- You should see the image of a house with a swimming pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651400d1",
   "metadata": {},
   "source": [
    "# Getting Started with this notebook\n",
    "- Create a new vector search enabled database in Astra.\n",
    "- Create a keyspace\n",
    "- Create a token with permissions to create tables\n",
    "- Download your secure-connect-bundle.zip file.\n",
    "- Download the [image files](https://github.com/difli/astra-vsearch-image/tree/19b7f1496627b8406b05d3abbea63968d92397f8/images)\n",
    "- When you open this notebook in Google Colab or your own notebook server, drag-and-drop the secure-connect-bundle.zip and images (one.jpg, two.jpg, three.jpg and for.jpg) into the File Browser of the notebook\n",
    "- Update the Environment Variables cell in the notebook with information from the token you generated, the name of your secure-connect-bundle file, keyspace, and table name you want to create."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311cc2e9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d88d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install cassandra-driver matplotlib sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae20e0b5",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2888e",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECURE_CONNECT_BUNDLE_PATH = 'secure-connect-vector-search-db.zip'\n",
    "ASTRA_CLIENT_ID = 'XXX'\n",
    "ASTRA_CLIENT_SECRET = 'XXX'\n",
    "KEYSPACE_NAME = 'vsearch'\n",
    "TABLE_NAME = 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e090e7",
   "metadata": {},
   "source": [
    "# Connect to Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c527a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_config = {\n",
    "   'secure_connect_bundle': SECURE_CONNECT_BUNDLE_PATH\n",
    "}\n",
    "auth_provider = PlainTextAuthProvider(ASTRA_CLIENT_ID, ASTRA_CLIENT_SECRET)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdb2ed",
   "metadata": {},
   "source": [
    "# Drop / Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating table {TABLE_NAME} in keyspace {KEYSPACE_NAME}\")\n",
    "session.execute(f\"CREATE TABLE IF NOT EXISTS {KEYSPACE_NAME}.{TABLE_NAME} (id int PRIMARY KEY, name TEXT, description TEXT, item_vector VECTOR<FLOAT, 512>)\")\n",
    "\n",
    "print(f\"Creating index image_ann_index on table {TABLE_NAME} and inserting example data\")\n",
    "session.execute(f\"CREATE CUSTOM INDEX IF NOT EXISTS image_ann_index ON {KEYSPACE_NAME}.{TABLE_NAME}(item_vector) USING 'StorageAttachedIndex'\")\n",
    "\n",
    "print(f\"Truncate table {TABLE_NAME} in keyspace {KEYSPACE_NAME}\")\n",
    "session.execute(f\"TRUNCATE TABLE {KEYSPACE_NAME}.{TABLE_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8251382e",
   "metadata": {},
   "source": [
    "# Load CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b048314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2d835",
   "metadata": {},
   "source": [
    "# Generate embeddings from images and load the table with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d94c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb1 = model.encode(Image.open('one.jpg'))\n",
    "img_emb2 = model.encode(Image.open('two.jpg'))\n",
    "img_emb3 = model.encode(Image.open('three.jpg'))\n",
    "img_emb4 = model.encode(Image.open('four.jpg'))\n",
    "\n",
    "image_data = [\n",
    "    (1, 'one.jpg', 'description1', img_emb1.tolist()),\n",
    "    (2, 'two.jpg', 'description2', img_emb2.tolist()),\n",
    "    (3, 'three.jpg', 'description3', img_emb3.tolist()),\n",
    "    (4, 'four.jpg', 'description4', img_emb4.tolist())\n",
    "]\n",
    "\n",
    "for image in image_data:\n",
    "    session.execute(f\"INSERT INTO {KEYSPACE_NAME}.{TABLE_NAME} (id, name, description, item_vector) VALUES {image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee54e61",
   "metadata": {},
   "source": [
    "# Generate embeddings from query string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a206036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_string = \"a house with a swimming pool\"\n",
    "text_emb = model.encode(query_string)\n",
    "print(f\"model provided embeddings for the string: 'a house with a swimming pool': {text_emb.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d1556e",
   "metadata": {},
   "source": [
    "# Vector search the image that shows a house with a swimming pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce0d88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f\"Astra DB vector search query: SELECT name, description, item_vector FROM {KEYSPACE_NAME}.{TABLE_NAME} ORDER BY item_vector ANN OF {text_emb.tolist()} LIMIT 1\")\n",
    "for row in session.execute(f\"SELECT name, description, item_vector FROM {KEYSPACE_NAME}.{TABLE_NAME} ORDER BY item_vector ANN OF {text_emb.tolist()} LIMIT 1\"):\n",
    "    print(\"\\t\" + str(row))\n",
    "    plt.title(row.name)\n",
    "    image = mpimg.imread(row.name)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8bcf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
